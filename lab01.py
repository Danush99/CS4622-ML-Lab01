# -*- coding: utf-8 -*-
"""Lab01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wOsEidr6JQ2aE7oOY4kStwGZPq0cBd2_
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np          # For mathematical calculations

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score
import seaborn as sns

import matplotlib.pyplot as plt  # For plotting graphs
from pandas import Series        # To work on series
# %matplotlib inline
import warnings                   # To ignore the warnings
warnings.filterwarnings("ignore")

train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/UOM/CS4622-ML/Lab 01/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/UOM/CS4622-ML/Lab 01/test.csv')
valid = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/UOM/CS4622-ML/Lab 01/valid.csv')

# Find columns with missing values and count how many missing values in each column
missing_columns = train.columns[train.isnull().any()]
missing_counts = train[missing_columns].isnull().sum()

# Print the columns with missing values and their corresponding missing value counts
print("shape of train: ", train.shape)
for column in missing_columns:
    print(f"Column '{column}' has {missing_counts[column]} missing values.")

L1 = "label_1" #Speaker ID
L2 = "label_2" #Speaker age
L3 = "label_3" #Speaker gender
L4 = "label_4" #Speaker accent
LABELS = [L1, L2, L3, L4,]
AGE_LABEL = L2
FEATURES = [f'feature_{i}' for i in range(1,257)]

#OutLier analysis
data = train[0:5]
# Set the style of seaborn
sns.set(style="whitegrid")

# Create a box plot for each feature to visualize outliers
for col in data.columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(data[col])
    plt.title(f'Box Plot of {col}')
    plt.xlabel(col)
    plt.show()

#Heat Map

sns.heatmap(train, annot=True, cmap='coolwarm', center=0)

train_df = train.copy()
test_df = test.copy()
valid_df = valid.copy()

train_df.head()

train_df[LABELS + [FEATURES[i] for i in range(0,256,32)]].describe()

train_df[L4].value_counts()

"""Label 2 has some NaN

Label 4 has large number of '6's (if we predict 6 then accuracy 70%) -> 6 has to undersample (reduce the weight of 6)
"""

train_df.info()

# from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler # eliminate outliers

x_train = {}
x_valid = {}
x_test = {}

y_train = {}
y_valid = {}
y_test = {}

#create dictionaries for each label
for target_label in LABELS:
  tr_df = train_df[train_df['label_2'].notna()] if target_label == "label_2" else train_df
  vl_df = valid_df[valid_df['label_2'].notna()] if target_label == "label_2" else valid_df
  te_df = test_df

  scaler = RobustScaler()
  # x_train_features = tr_df.drop(LABELS, axis=1)

  x_train[target_label] = pd.DataFrame(scaler.fit_transform(tr_df.drop(LABELS, axis=1)), columns=FEATURES)
  y_train[target_label] = tr_df[target_label]

  x_valid[target_label] = pd.DataFrame(scaler.transform(vl_df.drop(LABELS, axis=1)), columns=FEATURES)
  y_valid  [target_label] = vl_df[target_label]

  x_test[target_label] = pd.DataFrame(scaler.transform(te_df.drop(LABELS, axis=1)), columns=FEATURES)
  # y_test[target_label] = te_df[target_label] <- need to predict

y_train['label_2']

x_train_df = x_train[L4].copy()
y_train_df = y_train[L4].copy()

x_valid_df = x_valid[L4].copy()
y_valid_df = y_valid[L4].copy()

x_test_df = x_test[L4].copy()
# y_test__df = y_test[L1].copy() <- need to predict

from sklearn.utils import class_weight
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import numpy as np

# model train by Initialize kNN classifier
k = 5  # Number of neighbors
classifier = KNeighborsClassifier(n_neighbors=k)
# Train the kNN classifier on the selected features
classifier.fit(x_train_df, y_train_df)

# Accuarcy Check
y_pred = classifier.predict(x_valid_df)
# print("confusion_matrix:\n ",metrics.confusion_matrix(y_valid_df, y_pred))
print("accuracy_score: ",metrics.accuracy_score(y_valid_df, y_pred))
print("f1_score: ",f1_score(y_valid_df, y_pred, average='weighted')) #f-1 score
print("precision_score: ",metrics.precision_score(y_valid_df, y_pred, average='weighted' ))
print("recall_score: ",metrics.recall_score(y_valid_df, y_pred, average='weighted'))

# knn_conf_matrix = confusion_matrix(y_valid_df, y_pred)
# plt.figure(figsize=(15, 10))
# sns.heatmap(knn_conf_matrix, annot=True, fmt="d", cmap="Blues")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.title("Confusion Matrix from KNN")
# plt.show()

########### TEST ##############
y_test_pred = classifier.predict(x_test_df)

"""### **FEATURE ENGINEERING**"""

# @title 1️⃣ SelectKBest and f_classif for feature engineering
from sklearn.feature_selection import SelectKBest, f_classif, chi2
#chi2 (Chi-square) cannot be used when datafarame has negative numbers

#Validation
selector = SelectKBest(f_classif, k=150)
x_new = selector.fit_transform(x_train_df, y_train_df)
x_valid_df_after_f_classif = selector.transform(x_valid_df)
print("shape: ", x_new.shape)


# #Testing
x_test_df_after_f_classif = selector.transform(x_test_df)

# # model train
# clf = svm.SVC(kernel="linear")
# clf.fit( x_new, y_train_df)

# model train by Initialize kNN classifier
k = 5  # Number of neighbors
classifier = KNeighborsClassifier(n_neighbors=k)
# Train the kNN classifier on the selected features
classifier.fit(x_new, y_train_df)

# Accuarcy Check
y_pred = classifier.predict(x_valid_df_after_f_classif)
# print("confusion_matrix: ",metrics.confusion_matrix(y_valid_df, y_pred))
print("accuracy_score: ",metrics.accuracy_score(y_valid_df, y_pred))
print("f1_score: ",f1_score(y_valid_df, y_pred, average='weighted')) #f-1 score
print("precision_score: ",metrics.precision_score(y_valid_df, y_pred, average='weighted' ))
print("recall_score: ",metrics.recall_score(y_valid_df, y_pred, average='weighted'))

# knn_conf_matrix = confusion_matrix(y_valid_df, y_pred)
# plt.figure(figsize=(15, 10))
# sns.heatmap(knn_conf_matrix, annot=True, fmt="d", cmap="Blues")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.title("Confusion Matrix from KNN")
# plt.show()

# @title 2️⃣ Select Percentile for feature engineering
import pandas as pd
from sklearn.feature_selection import SelectPercentile, f_classif
from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

#Validation
# Initialize SelectPercentile with f_classif scoring and percentile=10 (adjust as needed)
selector = SelectPercentile(score_func=f_classif, percentile=40)
# Fit and transform the data
x_new = selector.fit_transform(x_new, y_train_df)
x_valid_df_after_selectPercentile = selector.transform(x_valid_df_after_f_classif)
print("shape: ", x_new.shape)

#Testing
x_test_df_after_selectPercentile = selector.transform(x_test_df_after_f_classif)

# model train by Initialize kNN classifier
k = 5  # Number of neighbors
classifier = KNeighborsClassifier(n_neighbors=k)
# Train the kNN classifier on the selected features
classifier.fit(x_new, y_train_df)

# Accuarcy Check
y_pred = classifier.predict(x_valid_df_after_selectPercentile)
# print("confusion_matrix: ",metrics.confusion_matrix(y_valid_df, y_pred))
print("accuracy_score: ",metrics.accuracy_score(y_valid_df, y_pred))
print("f1_score: ",f1_score(y_valid_df, y_pred, average='weighted')) #f-1 score
print("precision_score: ",metrics.precision_score(y_valid_df, y_pred, average='weighted' ))
print("recall_score: ",metrics.recall_score(y_valid_df, y_pred, average='weighted'))

# knn_conf_matrix = confusion_matrix(y_valid_df, y_pred)
# plt.figure(figsize=(15, 10))
# sns.heatmap(knn_conf_matrix, annot=True, fmt="d", cmap="Blues")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.title("Confusion Matrix from KNN")
# plt.show()

# @title 3️⃣ PCA for feature engineering
from sklearn.decomposition import PCA

#Validation
pca = PCA(n_components=0.98, svd_solver='full')
pca.fit(x_new)
x_train_trf = pd.DataFrame(pca.transform(x_new))
x_valid_trf_pca = pd.DataFrame(pca.transform(x_valid_df_after_selectPercentile))
print('Shape after PCA: ',x_train_trf.shape)

# model train by Initialize kNN classifier
k = 5  # Number of neighbors
classifier = KNeighborsClassifier(n_neighbors=k)
# Train the kNN classifier on the selected features
classifier.fit(x_train_trf, y_train_df)

# Accuarcy Check
y_pred = classifier.predict(x_valid_trf_pca)
# print("confusion_matrix:\n ",metrics.confusion_matrix(y_valid_df, y_pred))
print("accuracy_score: ",metrics.accuracy_score(y_valid_df, y_pred))
print("f1_score: ",f1_score(y_valid_df, y_pred, average='weighted')) #f-1 score
print("precision_score: ",metrics.precision_score(y_valid_df, y_pred, average='weighted' ))
print("recall_score: ",metrics.recall_score(y_valid_df, y_pred, average='weighted'))

knn_conf_matrix = confusion_matrix(y_valid_df, y_pred)
plt.figure(figsize=(15, 10))
sns.heatmap(knn_conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix from KNN")
plt.show()

#Testing
x_test_df_after_pca = pd.DataFrame(pca.transform(x_test_df_after_selectPercentile))
y_test_predict_after_feature_engineering = classifier.predict(x_test_df_after_pca)
print('x_test_df_after_pca Shape after PCA: ',x_test_df_after_pca.shape)
print('y_test_predict_after_feature_engineering Shape after PCA: ',y_test_predict_after_feature_engineering.shape)

knn_predictions_before_feature_engineering = y_test_pred
knn_predictions_after_feature_engineering = y_test_predict_after_feature_engineering
no_of_new_features = x_test_df_after_pca.shape[1]

df_for_csv = pd.DataFrame()
df_for_csv['Predicted labels before feature engineering'] = knn_predictions_before_feature_engineering
df_for_csv['Predicted labels after feature engineering'] = knn_predictions_after_feature_engineering
df_for_csv['No of new features'] = no_of_new_features

# x_test_df_after_pca.head()
# print(x_test_df_after_pca[:, 5])

for i in range(x_test_df.shape[1]):
    df_for_csv[f'new_feature_{i+1}'] = x_test_df_after_pca.iloc[:, i] if i < x_train_trf.shape[1] else pd.NA

df_for_csv.head()
df_for_csv.to_csv('/content/drive/MyDrive/Colab Notebooks/UOM/CS4622-ML/Lab 01/190232V_label_4.csv', index=False)

# @title 2️⃣ VarianceThreshold for feature engineering
from sklearn.feature_selection import VarianceThreshold

threshold = 0.55
selector = VarianceThreshold(threshold=threshold)
x_new = selector.fit_transform(x_train_df)
x_valid_df_after_VarianceThreshold = selector.transform(x_valid_df)
print("shape: ", x_new.shape)

# model train
clf = svm.SVC(kernel="linear")
clf.fit( x_new, y_train_df)

# Accuarcy Check
y_pred = clf.predict(x_valid_df_after_VarianceThreshold)
print(metrics.confusion_matrix(y_valid_df, y_pred))
print(metrics.accuracy_score(y_valid_df, y_pred))
print(metrics.precision_score(y_valid_df, y_pred, average='weighted' ))
print(metrics.recall_score(y_valid_df, y_pred, average='weighted'))

# @title 3️⃣ Backward Feature Elimination using logisticregression for feature engineering
#Backward Feature Selection # - not working
from sklearn.linear_model import LogisticRegression
from mlxtend.feature_selection import SequentialFeatureSelector
lr = LogisticRegression(class_weight='balanced', solver='lbfgs', random_state=42, n_jobs=-1, max_iter=500)
lr.fit(x_train[L1], y_train[L1])
bfs = SequentialFeatureSelector(lr, k_features='best', forward=False, n_jobs=-1)
bfs.fit(x_train[L1], y_train[L1])
features = list(bfs.k_feature_names_)
features = list(map(int, features))
lr.fit(x_train[features], y_train)
y_pred = lr.predict(x_train[features])

# @title 4️⃣ Random Forest feature Importance for feature engineering
from sklearn.ensemble import RandomForestClassifier

#create the random forest with your hyperporoneters.
model = RandomForestClassifier()

# fit the model to start training.
model.fit(x_train[L1], y_train[L1])

#get the importance of the resulting features.
importances = model.feature_importances_

#create a data frame for visualization.
final_df = pd.DataFrame({"Features": pd.DataFrame(x_train[L1]).columns, "importances":importances})
final_df.set_index('importances')

#sort in ascending order to better visualization.
final_df = final_df.sort_yalues('importances')

#plot the feature importances in bars.
final_df.plot.bar(color = 'teal')

# @title 6️⃣ Reducing correlation for feature engineering
import matplotlib.pyplot as plt
import seaborn as sns

corr_matrix = x_train_df.corr()
corr_threshold = 0.5
filterred_corr_matrix = corr_matrix[ (corr_matrix > corr_threshold) | (corr_matrix < -corr_threshold) ]
plt.figure(figsize=(10,8))
sns.heatmap(filterred_corr_matrix, annot=True, cmap='coolwarm', center=0)